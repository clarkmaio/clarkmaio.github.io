{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ciau","text":"<p>i am Andrea, you can call me Andrea (or clarkmaio)  i work as data scientist but I am a mathematician</p> <p> </p>"},{"location":"#favorite-books","title":"favorite books","text":"<ul> <li>Guns, Germs and Steel: The Fates of Human Societies, Jered Diamond</li> <li>The Gospel According to Jesus Christ, Jose Saramago</li> <li>Slam Dunk, Takehiko Inoue</li> </ul>"},{"location":"#favorite-boardgames","title":"favorite boardgames","text":"<ul> <li>Power Grid</li> <li>Secret Hitler</li> </ul>"},{"location":"pages/blog/","title":"Blog","text":""},{"location":"pages/blog/#2023","title":"2023","text":""},{"location":"pages/blog/#dunk","title":"Dunk","text":""},{"location":"pages/blog/#2022","title":"2022","text":""},{"location":"pages/blog/#maximum-entropy-priors","title":"Maximum entropy priors","text":""},{"location":"pages/blog/subpages/dunk/","title":"Dunk","text":"<p>So here the thing, in the last period I've started reading Slam Dunk. It is a very nice comic.  Of course I use to watch the cartoon on MTV when I was a boy but now I am really into the books, and I am really fascinated by the whole story and the characters... but I don't want to talk about it.</p> <p>The only thing that matter is that it really motivated me to train in order to do a slam dunk: namely I am going to train to jump high.</p> <p>This will be a dynamic post which means that it will change during time. Infact I will report my improments of my training with few tables and plots.</p> <p>I hope this will help my ego and my self to not stop the training.</p>"},{"location":"pages/blog/subpages/maximum_entropy_priors/","title":"Maximum entropy priors","text":"<p>In this period I am studying a lot about Bayesian statistics. The focus is of course on the applications, in particular using PyMC package on Python.</p> <p>Ofcourse one of the most common question when studying Bayesian statistics is: Which prior I am suppose to use? You want to inject as much info as possible, but usually you have very few information to inject.</p>"},{"location":"pages/hobbies/","title":"Index","text":""},{"location":"pages/hobbies/#photography","title":"photography","text":""},{"location":"pages/hobbies/#amigurumi","title":"amigurumi","text":""},{"location":"pages/hobbies/#dunk","title":"dunk","text":""},{"location":"pages/hobbies/subpages/amigurumi/","title":"Amigurumi","text":""},{"location":"pages/hobbies/subpages/amigurumi/#legend","title":"Legend","text":"<pre><code>sc: single crochet\ninc: increase\ndec: decrease\nBLO: back-loop only\nFLO: fron-loop only\n</code></pre>"},{"location":"pages/hobbies/subpages/amigurumi/#sphere","title":"Sphere","text":"<p> <pre><code>1. 6sc (magic circle) [6]\n2. (inc) x 6 [12]\n3. (sc, inc) x 6 [18]\n4. sc, inc, (2sc, inc) x 5, sc [24]\n5. (3sc, inc) x 6 [30]\n6-7. 30sc [30]\n8. 2sc, inc, (4sc, inc) x 5, 2sc [36]\n9-11. 36sc [36]\n12. 2sc, dec, (4sc, dec) x 5, 2sc [30]\n13-14. 30sc [30]\n15. (3sc, dec) x 6 [24]\n16. sc, dec, (2sc, dec) x 5, sc [18]\n17. (sc, dec) x 6 [12]\n</code></pre></p>"},{"location":"pages/hobbies/subpages/amigurumi/#pot-by-cutiepiecrochetcom","title":"Pot (by cutiepiecrochet.com)","text":"<p> <pre><code>1. 6sc (magic circle) [6]\n2. (inc) x 6 [12]\n3. (sc, inc) x 6 [18]\n4. sc, inc, (2sc, inc) x 5, sc [24]\n5. (3sc, inc) x 6 [30]\n6. 30 BLO sc [30]\n7. 30sc [30]\n8. (9sc, inc) x 3 [33]\n9-10. 33sc [33]\n11. 5sc, inc, (sc 10, inc) x 2, 5sc [36]\n12-14. 36sc [36]\n15. FLO (sc 11, inc)x3 [39]\n16. BLO sc around [39]\n17. sc around through the stitches in round 16 and the back loops of round 15\n18. 30sc [39]\n</code></pre></p>"},{"location":"pages/hobbies/subpages/dunk/","title":"dunk","text":"<p>I want dunk.</p>"},{"location":"pages/hobbies/subpages/dunk/#jump-height","title":"jump height","text":"Date cm 01/01/01 10"},{"location":"pages/hobbies/subpages/dunk/#max-box-squat","title":"max box squat","text":"Date Kg 01/01/01 110"},{"location":"pages/hobbies/subpages/photo/","title":"photography","text":"<p>I use a Canon AE1-Program with 50mm lens.</p> <ul> <li> <p> Kungsladen</p> <p>Photos of my 2 hiking trips in the Kungsladen. </p> <p> Have a look</p> </li> <li> <p> Carneval of Ivrea</p> <p>Pretty unique folcloristic event that take place during carneval in Ivrea (Italy). I've never seen so much violence, happiness and oranges in the same place.</p> <p> Have a look</p> </li> </ul>"},{"location":"pages/hobbies/subpages/photo/photo_ivrea/","title":"Carneval of Ivrea 2024","text":"<p>"},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/","title":"Kungsladen","text":""},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/#abisko-nikkaluotta-2023","title":"Abisko  Nikkaluotta [2023]","text":""},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/#abisko-hemaven-2024","title":"Abisko  Hemaven [2024]","text":""},{"location":"pages/projects/","title":"Index","text":""},{"location":"pages/projects/#essentials","title":"essentials","text":""},{"location":"pages/projects/#template","title":"template","text":""},{"location":"pages/projects/#ooops-index","title":"ooops index","text":""},{"location":"pages/projects/subpages/essentials/","title":"Introduction","text":"<p>We italians, whenever we struggle accomplish a complicated task, are used to compose small \"poetries\" (2/3 words long, similar to japenese haiku).  These poetries consist in a sort of dialog we have with God.</p> <p>We call these italian haiku bestemmie.</p> <p>  A couple of weeks ago I started studing kedro, a package to produce programs by McKinsey.</p> <p>I was interested in the way this package handles dataset and the concept of nodes and pipelines to make your code clean and well organised.</p> <p>But I was amazed by the interactive visualisation tool they've implemented to visualize the structure of the program and the modules that compose it.</p> <p>For few weeks I force my self to use this package; I really wanted to use that visualisation tool on daily basis.  I struggled a lot. I soon realized how rigid the package was, how difficult it was to add even a little of customisation and the amount of black box functions I was supposed to call not understanding what they were doing under the hood just to make it works.</p> <p> </p> <p>The final result is that: * I gave up kedro due to frustration. * I've created my own package to replicate kedro's objects I like the most appling some changes to improve them. * I can now compose bestemmie  up to 10 words long, engaging multiple deities even from different religions!</p> <p> </p>"},{"location":"pages/projects/subpages/essentials/#the-package","title":"The package","text":"<p>In this page I will illustrate what I've implemented guiding with examples and short explainations.</p> <p>Info</p> <p>You can find this tool box in the github repository clarkpy_essentials. The package can be installed with the command:</p> <pre><code>pip install git+https://github.com/clarkmaio/clarkpy_essentials.git\n</code></pre>"},{"location":"pages/projects/subpages/essentials/#parsermanager","title":"ParserManager","text":"<p>Let's start from a dummy one: the <code>ParserManager</code>.</p> <p>I just do not like to edit parser from a <code>.py</code> file</p> <p><code>ParserManager</code> is just a class that parse a <code>yaml</code> file and build a parser using the standard library <code>argparser</code>.</p> <p>Parser keys will be stored in a dictionary ready to be queried.</p> <pre><code># Example of parser_conf.yml\nvariable_int:\n  default: 9\n  help: 'This variable is an integer'\n\nvariable_str:\n  default: abc\n  help: 'This variable is a string'\n\nvariable_nargs:\n  default: ['a', 'b', 'c']\n  nargs: '*'\n</code></pre> <pre><code>from clarkpy_essentials import ParserManager\n\nPARSER_CONF = `/path_to_parser/parser_conf.yml`\nparser_args = ParserManager.load(path = PARSER_CONF)\n\n# parser_args is now just a dictionary\nprint('variable_int', parser_args['variable_int'])\nprint('variable_nargs', parser_args['variable_nargs'])\n</code></pre>"},{"location":"pages/projects/subpages/essentials/#datacatalog","title":"DataCatalog","text":"<p>This class is nothing but an huge if/else function to handle different type of dataset.</p> <p>The idea has been stolen directly from kedro. In kedro the user write down a yaml file to map dataset to a label. The dataset can be then loaded in pipeline simply using the label as input of nodes.</p> <p>Since I was looking for something more explicit and a tool that could be used outside a kedro pipeline I've implemented the <code>DataCatalog</code> class.</p> <p>The most important thing to use this class is to write properly the data catalog.</p> <p>It is important to specify for each label the parameters:</p> <ul> <li><code>type</code>: namely the way the dataset will be loaded. Can be <code>pandas.csv</code>, <code>pandas.excel</code>, <code>pandas.hdf</code>, <code>pandas.parquet</code>, <code>polars.csv</code>, <code>polars.parquet</code>,<code>yaml</code>, <code>json</code>, <code>torch</code>, <code>pickle</code></li> <li><code>filepath</code>: the path to the file. By default this is a relative path that will be appended to <code>source_path</code> defined in the constructor. You can refer to an absolute path using the prefix <code>@abs:</code></li> <li><code>load_kwargs</code>: additional paramters that will be passed to loader fucntion as kwargs</li> </ul> <pre><code># Example of a data catalog yml\nPandasDataset:\n    type: pandas.csv\n    filepath: dataset/csv_dataset.csv\n    load_kwargs:\n        sep: ','\n\nPolarsDataset:\n    type: polars.parquet\n    filepath: @abs:/home/user/dataset/parquet_dataset.parquet\n\nUrlPandasDataset:\n    type: pandas.csv\n    filepath: @abs:https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv\n    load_kwargs:\n        sep: ','\n</code></pre> <pre><code>from clarkpy_essentials import DataCatalog\n\n# Initialize the datacatalog specifying the catalog.yml path and the source path that will be used to load datasets.\nCATALOG_PATH = '/home/user/catalog.yml'\ndc = DataCatalog(catalog=CATALOG_PATH)\n\nprint('Pandas Dataset', dc('PandasDataset').head(5))\nprint('Polars Dataset', dc('PolarsDataset').head(5))\nprint('URL Pandas Dataset', dc('UrlPandasDataset').head(5))\n</code></pre> <p>You can even add a custom loader function using the method:</p> <pre><code>def myloaderfunc(filepath: str, **kwargs):\n    ...\n    load file\n    ...\n    return file\n\nfrom clarkpy_essentials import DataCatalog\ndc = DataCatalog(catalog=CATALOG_PATH)\ndc.add_custom_loader(key='myloader', func=myloaderfunc)\n</code></pre> <p>Now you can load data with label <code>'mylader'</code> using function <code>myloaderfunc</code>.</p>"},{"location":"pages/projects/subpages/essentials/#context","title":"Context","text":"<p>This is a very trivial class. The only purpose it to store senttings and variables as properties of a class.</p> <p>Whatever you pass to the constructor of this class will be trasformed in a property.</p> <p>Note</p> <p><code>catalog</code> is a privileged key that is ment to store a <code>DataCatalog</code> instance. It is privileged in the sense that whatever is stored in this key will be treated as <code>DataCatalog</code> intance in <code>Pipeline</code> (see below).</p> <pre><code>from clarkpy_essentials import Context\n\nglobal_vars = {'GLOBAL': 100}\ncontext = Context(parser=parser_args, global_vars=global_vars, catalog=dc)\n\nprint(context.parser)\nprint(context.global_vars)\nprint(context.catalog)\n</code></pre>"},{"location":"pages/projects/subpages/essentials/#pipelines-and-nodes","title":"Pipelines and nodes","text":"<p>Again I've tried to copy a feature of kedro package but tring to make it easier to use it.</p> <p>I really like the way Pipelines and Nodes work in kedro, in particular the elegant way to pass input/output across different nodes.</p> <p>You can use mine <code>Pipeline</code> and <code>Node</code> classes to accomplish the same but you can easily use them in your program without using kedro.</p> <p>Here a dummy example:</p> <pre><code>from clarkpy_essentials import Pipeline, Node, Context\n\n# ------------ Node fucntions --------------\ndef prod(x: float, y: float) -&gt; float:\n  return x*y\n\n\ndef sum(x: float, y: float) -&gt; float:\n  reutrn x+y\n\n\n# ------------ Create Context --------------\nGLOBAL_VARIABLES = {'var1': 1, 'var2': 100, 'var3': 10}\ncontext = Context(global_variables=GLOBAL_VARIABLES)\n\n# ------------ Initialize Pipeline ---------\npipeline = Pipeline([\n    Node(func=prod,\n         inputs=['context.global_variables.var1', 'context.global_variables.var2'],\n         outputs='output_prod'),\n    Node(func=sum,\n         inputs=['output_prod', 'context.global_variables.var3'],\n         outputs='outpout_sum')\n])\n\n# ------------ Run Pipeline ----------------\npipeline_results = pipeline.run(context=context)\nprint('All pipeline variables', pipeline_results)\n</code></pre> <p>For more complex example have a look at one of mine template.</p>"},{"location":"pages/projects/subpages/essentials/#datatransformer","title":"DataTransformer","text":"<p>When dealing with multiple models it happens that different data processing are needed depending on the model you are using.</p> <p>To make it easier to control this operations I've created a class that take in input a list of instructions and transform a dataset accordingly.</p> <p>Ideally you should create instructions (in the form of a <code>yaml</code>) for each model.</p> <p>To use correctly this class you should:</p> <ol> <li> <p>Write the functions to transform data like so:</p> <pre><code>def transformer_1(X, **kwars):\n    transform X some how\n    return X\n\ndef transformer_2(X, **kwars):\n    transform X some how\n    return X\n</code></pre> </li> <li> <p>Create an instance of <code>DataTransformer</code> and map the functions:</p> <pre><code>dt = DataTransformer({\n    't1': trabsformer_1,\n    't2': transformer_2 \n})\n</code></pre> </li> <li> <p>Create list of instructions.</p> <p>Each instruction step must consist in a dictionary containing keys:</p> <ul> <li><code>type</code>: label of the function to apply </li> <li><code>kwargs</code> [optional]: function kwargs</li> </ul> <pre><code>instructions = [\n    {\n        'type': 't1'\n        'kwargs': {\n            'kwarg1': ...,\n            'kwarg2': ...\n        }\n    },\n\n    'type': 't2'\n        'kargs': {\n            'kwarg1': ...,\n            'kwarg2': ...\n        }\n]\n</code></pre> </li> <li> <p>Transform data <pre><code>X_transformed = dt.transform(X, instructions=instructions)\n</code></pre></p> </li> </ol> <p>Here a complete example:</p> <pre><code>def f1(X):\n    return X\n\ndef f2(X, alpha):\n    return X*alpha\n\ndt = DataTransformer()\ndt.add_transformer_catalog({\n    'identity': f1,\n    'mul': f2\n})\n\n\ninstructions = [\n    {\n        'type': 'identity'\n    },\n\n    {\n        'type': 'mul',\n        'kwargs': {\n            'alpha': 2\n        }\n    }\n]\n\n\nX = [1,2,3,4,5]\nX_transformed = dt.transform(X=X, instructions=instructions)\nprint(X_transformed)\n</code></pre>"},{"location":"pages/projects/subpages/essentials/#decorators","title":"Decorators","text":"<p>Just a collection of decorators you can import from <code>clarkpy_essentials.decorators</code>.</p> <ul> <li><code>@force_kwargs</code>:  raise an error if function arguments are not passed through keys.</li> <li><code>@deepcopy_args</code>: deep copy args and kwargs passed to a function (usefully to avoid operations on pandas dataframe copy to affect original one).</li> </ul>"},{"location":"pages/projects/subpages/ooops_index/","title":"Ooops index","text":"<p>\ud835\udddc \ud835\uddf8\ud835\uddfb\ud835\uddfc\ud835\ude04 \ud835\uddfb\ud835\uddfc\ud835\ude01\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddef\ud835\uddfc\ud835\ude02\ud835\ude01 \ud835\ude03\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf2\ud835\uddf9\ud835\ude00 \ud835\uddfc\ud835\uddff \ud835\uddfa\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf3\ud835\uddf3\ud835\uddf6\ud835\uddf0.</p> <p>The only thing that I've understood at the end of 2024 is that the Baltic sea is an interesting place where it could happen that vessels try to plow the seabed using the anchor as if they were planting wheat in preparation for a famine.</p> <p>Just for fun (mostly to learn how to deploy on HuggingFace a stramlit dashbord whose source code is in a private repository) I've create a dashboard that shows how many \"suspicious\" vessels are around Estlink 1 and 2 in a given hour in time.</p>"},{"location":"pages/projects/subpages/ooops_index/#have-a-look-at-the-ooops-index-dashboard","title":"Have a look at  the Ooops index dashboard!","text":""},{"location":"pages/projects/subpages/ooops_index/#resources","title":"Resources","text":"<ul> <li>Christmas harvest</li> <li>Ooops Dashboard source code</li> <li>Ooops dataset</li> <li>Fintraffic API</li> </ul>"},{"location":"pages/projects/subpages/template/","title":"Template","text":"<p>Warning</p> <p>This is post is really boring but fucking useful</p> <p>As Data Scientist my job conist at the end to formalize a problem that was told be about (or something that I found useful to solve), translate it in a mathematical framework and then use a programming language to solve it numerically.</p> <p>The most interesting part is of course to create the core mathematical model, i.e. the part of the process in which I am asked to produce something original (or draw on my background and knoweldge to produce a solution).</p> <p>The reality is that whatever I produce as mathematical model at the end must be translated in a program that usually must run periodically and business more and more rely on its output and on the fact that it will always work</p> <p></p> <pre><code>flowchart LR\n    A([Problem!])\n    B([Mathematical formalisation])\n    C([Requirements])\n    D([Programming solution])\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; |world changes|A\n\n\n    style A fill:red, stroke:white, color: white, font-weight: bold;\n    style B fill:lightblue, stroke:white, color: white, font-weight: bold;\n    style C fill:gray, stroke:white, color: white, font-weight: bold;\n    style D fill:lightgreen, stroke:white, color: white, font-weight: bold;</code></pre> <p> </p> <p>I am not a SE and my heart belongs to math but in my small experience I've realised pretty soon how much time the step Programming solution can take both when you write the program the first time and in particular to mantain and expand the application itself.</p> <p>Since I am really lazy and I really wanted to minimize the time spent of these boring step I end up creating a cookiecutter template for my programs (mostly timeseries forecasting but the template is pretty general).</p> <p>The purpose of such template is pretty obvious: * Less code to write (less repetitive job) * Every program will looks like the same so that I will never be lost.</p> <p>Info</p> <p>The template is tailor made to work with modules I wrote to make my programs more clean and clear.</p> <p>You can find these modules in the git repository:</p> <p>https://github.com/clarkmaio/clarkpy_essentials</p> <p>99% of the philosophy behind this template is still valid and can be adopted in any case.</p>"},{"location":"pages/projects/subpages/template/#structure","title":"Structure","text":"<p>Let's see the structure first and few comments on the core parts of this template</p> <pre><code>MyProgram/\n\u251c\u2500\u2500 conf/\n\u2502   \u251c\u2500\u2500 catalog.yml\n\u2502   \u251c\u2500\u2500 config.yml\n\u2502   \u2514\u2500\u2500 parser.yml\n\u251c\u2500\u2500 dataset\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 notebooks\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __main__.py\n\u2502   \u251c\u2500\u2500 data_loader\n\u2502   \u251c\u2500\u2500 data_transformer\n\u2502   \u251c\u2500\u2500 model\n\u2502   \u251c\u2500\u2500 nodes\n\u2502   \u2514\u2500\u2500 pipeline/\n\u2502       \u251c\u2500\u2500 backtest.py\n\u2502       \u251c\u2500\u2500 gridsearch.py\n\u2502       \u251c\u2500\u2500 predict.py\n\u2502       \u2514\u2500\u2500 train.py\n\u251c\u2500\u2500 submodules\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"pages/projects/subpages/template/#but-why","title":"But... why?","text":"<p>Now I will explain each part of this template and what is the purpose of the main modules.</p>"},{"location":"pages/projects/subpages/template/#main","title":"main","text":"<p>The entry point for every functionality of the program is <code>__main__.py</code>.  It is the only script you are sopposed to launch.</p> <pre><code>cd MyProgram\npython src/__main__.py --key1 arg1 --key2 arg2 ...\n</code></pre> <p>Two things will happen in the main function: * Initialisation of context (see below) * Run pipeline depending of user request</p>"},{"location":"pages/projects/subpages/template/#conf","title":"conf","text":"<p>In <code>conf</code> folder are stored all the settings file: * <code>config.yml</code>: global variables. Those variables that are supposed to never change. * <code>parser.yml</code>: variables that user must initialize via argument parser. </p> <pre><code>These keys namely determine whih pipeline will run and variables that user need to change easily.\n</code></pre> <ul> <li> <p><code>catalog.yml</code>: this file is ment to be used with <code>DataCatalog</code> class of <code>clarkpy_essentials</code> but it can be useful even if you are not using this tool.</p> <p>In this file are stored path to dataset, instructions about how these dataset should be loaded and sql queries.</p> <p>Each dataset is mapped to a label so that it will be extremely easy and clean to load the datasets when using <code>Pipeline</code> syntax.</p> </li> </ul>"},{"location":"pages/projects/subpages/template/#src","title":"src","text":"<p>This is the folder where you will find core functions.</p> <p>My idea consist simply in writing a pipeline function (eventually using the <code>Pipeline</code> class) in <code>pipeline</code> folder for every functionality of the program.</p> <p>The rest of the code base is a support for these functions.</p> <ul> <li><code>data_loader</code>: functions to collect/merge data and eventually loading them in case you are not using <code>DataCatalog</code>.</li> <li> <p><code>data_transform</code>: collection of functions to transform data. Also this part can be simplified a lot using <code>DataTransformer</code> from <code>clarkpy_essentials</code>. </p> <p>See the github page of the repository for examples.</p> </li> <li> <p><code>model</code>: simple class to store model class and relative configuration files.</p> <p>I usually have one <code>yml</code> file for each model.</p> </li> <li> <p><code>nodes</code>: this is the folder where I store atomic functions that are building blocks of my pipelines.</p> <p>At the end pipelines will be nothing but a sequantial script that call a set of nodes.</p> </li> </ul> <pre><code>---\ntitle: Template\n---\n\nflowchart TD\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    %%% STYLES CLASS\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    classDef doc stroke: white, fill: lightgray\n    classDef pipeline fill: lightgreen, color: white, stroke: white, font-weight: bold\n    classDef main fill: blue, font-weight: bold, color: white, stroke: white, font-size: 30px\n    classDef coreclass fill: red, font-weight: bold, color: white, stroke: white, font-size: 20px\n    classDef icon fill: white, stroke: black\n    classDef folder fill: orange, font-weight: bold, color: white, stroke: white, font-size: 30px\n\n\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    %%% NODES\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    main[main]:::main\n\n    catalog_file[catalog.yml]:::doc\n    catalog_class([DataCatalog])\n\n    config_file[config.yml]:::doc\n\n    parser_file[parser.yml]:::doc\n    parser_class([ParserManager])\n\n    logger_class([Logger])\n\n    context([Context]):::coreclass\n\n    nodes[Nodes]:::folder\n    %%%data_transformer[Data transformer]:::folder\n    %%%data_loader[Data loader]:::folder\n\n    train_pipeline[Train pipeline]:::pipeline\n    gridsearch_pipeline[Gridsearch pipeline]:::pipeline\n    backtest_pipeline[Backtest pipeline]:::pipeline\n    predict_pipeline[Predict pipeline]:::pipeline\n\n    database(\"&lt;img src='https://www.svgrepo.com/show/522832/database.svg'; width='40'&gt;\"):::icon\n    experiment_tracking(\"&lt;b&gt; Experiment tracking&lt;/b&gt; &lt;img src='https://www.svgrepo.com/show/460996/dashboard-alt-3.svg'&gt;\"):::icon\n\n    model[trained model]:::doc\n\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    %%% DIAGRAM\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n    catalog_file -.-&gt; catalog_class\n    parser_file -.-&gt; parser_class\n\n    catalog_class --&gt; context\n    parser_class --&gt; context\n    config_file -.-&gt; context\n    config_file -.-&gt; logger_class\n    logger_class --&gt; context\n    context --&gt; main\n\n    main ---&gt; train_pipeline\n    main ---&gt; predict_pipeline\n    main ---&gt; gridsearch_pipeline\n    main ---&gt; backtest_pipeline\n\n    train_pipeline -.-&gt; model\n\n    %%%train_pipeline --&gt; database\n    gridsearch_pipeline ---&gt; database\n    backtest_pipeline ---&gt; database\n    predict_pipeline ---&gt; database\n\n\n    train_pipeline ----&gt; experiment_tracking\n    %%%gridsearch_pipeline --&gt; experiment_tracking\n    %%%predict_pipeline ---&gt; experiment_tracking\n    backtest_pipeline ----&gt; experiment_tracking\n</code></pre>"},{"location":"pages/til/","title":"Today I learned","text":"<p>01.01.2023 This is a test</p>"},{"location":"pages/til/subpages/20230101_test/","title":"Sa sa prova","text":"<p>Today I learned how to create a TIL post and create a link from TIL page to the post it self. It was not that hard.</p> <p>For sure I have to work on the design.</p>"},{"location":"pages/til/subpages/20241017_priors_and_cov/","title":"Priors and CoV","text":"<p>One of the most difficult aspect when dealing with Bayesian models is the choice of priors.</p> <p>Infact it is never clear which priors you should adopt and different priors can lead to very different results.</p> <p>I've found out that one can actually follow a simple procedure to find the \"best\" priors given (if any) the informations one has about them.</p> <p> But what does it mean that a prior is the best one?</p> <p>By best I mean that it is rapresentative exactly of the informations you have, nothing more, nothing less.</p> <p>Namely one would inject in the priors a precise information and avoi choosing a distribution that, implicitly, bring additional, not wanted informations.</p> <p>For example if I am looking for a prior to rapresent a standard deviation and I only know that it must be a positive value then choosing as prior an uniform distribution $U(0,10)$ means that I am implicitly injecting an additional costraint $\\sigma&lt;10$</p>"},{"location":"pages/til/subpages/20241017_priors_and_cov/#entropy","title":"Entropy","text":""},{"location":"pages/til/subpages/20241017_priors_and_cov/#minimization-problem","title":"Minimization problem","text":""},{"location":"pages/useful%21/","title":"Useful!","text":""},{"location":"pages/useful%21/#people","title":"People","text":"<ul> <li>koaning the idea of this website and the website itself have been copied from this man</li> <li>m-clark GAM God</li> <li>ritchie vink</li> <li>robj hyndman</li> <li>daniel linssen amazing game developer</li> <li>pesciolini patatini veeeeeeeeery nice paints</li> </ul>"},{"location":"pages/useful%21/#links","title":"Links","text":"<ul> <li>matplotlib tricks</li> </ul>"}]}