{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ciau i am Andrea, you can call me Andrea (or clarkmaio) i work as data scientist but I am a mathematician favorite books Guns, Germs and Steel: The Fates of Human Societies, Jered Diamond The Gospel According to Jesus Christ, Jose Saramago Slam Dunk, Takehiko Inoue favorite boardgames Power Grid Secret Hitler","title":"ciau"},{"location":"#ciau","text":"i am Andrea, you can call me Andrea (or clarkmaio) i work as data scientist but I am a mathematician","title":"ciau"},{"location":"#favorite-books","text":"Guns, Germs and Steel: The Fates of Human Societies, Jered Diamond The Gospel According to Jesus Christ, Jose Saramago Slam Dunk, Takehiko Inoue","title":"favorite books"},{"location":"#favorite-boardgames","text":"Power Grid Secret Hitler","title":"favorite boardgames"},{"location":"pages/blog/","text":"Blog 2023 Dunk 2022 Maximum entropy priors","title":"Blog"},{"location":"pages/blog/#blog","text":"","title":"Blog"},{"location":"pages/blog/#2023","text":"","title":"2023"},{"location":"pages/blog/#dunk","text":"","title":"Dunk"},{"location":"pages/blog/#2022","text":"","title":"2022"},{"location":"pages/blog/#maximum-entropy-priors","text":"","title":"Maximum entropy priors"},{"location":"pages/blog/subpages/dunk/","text":"Dunk So here the thing, in the last period I've started reading Slam Dunk. It is a very nice comic. Of course I use to watch the cartoon on MTV when I was a boy but now I am really into the books, and I am really fascinated by the whole story and the characters... but I don't want to talk about it. The only thing that matter is that it really motivated me to train in order to do a slam dunk: namely I am going to train to jump high. This will be a dynamic post which means that it will change during time. Infact I will report my improments of my training with few tables and plots. I hope this will help my ego and my self to not stop the training.","title":"Dunk"},{"location":"pages/blog/subpages/dunk/#dunk","text":"So here the thing, in the last period I've started reading Slam Dunk. It is a very nice comic. Of course I use to watch the cartoon on MTV when I was a boy but now I am really into the books, and I am really fascinated by the whole story and the characters... but I don't want to talk about it. The only thing that matter is that it really motivated me to train in order to do a slam dunk: namely I am going to train to jump high. This will be a dynamic post which means that it will change during time. Infact I will report my improments of my training with few tables and plots. I hope this will help my ego and my self to not stop the training.","title":"Dunk"},{"location":"pages/blog/subpages/maximum_entropy_priors/","text":"Maximum entropy priors In this period I am studying a lot about Bayesian statistics. The focus is of course on the applications, in particular using PyMC package on Python. Ofcourse one of the most common question when studying Bayesian statistics is: Which prior I am suppose to use? You want to inject as much info as possible, but usually you have very few information to inject.","title":"Maximum entropy priors"},{"location":"pages/blog/subpages/maximum_entropy_priors/#maximum-entropy-priors","text":"In this period I am studying a lot about Bayesian statistics. The focus is of course on the applications, in particular using PyMC package on Python. Ofcourse one of the most common question when studying Bayesian statistics is: Which prior I am suppose to use? You want to inject as much info as possible, but usually you have very few information to inject.","title":"Maximum entropy priors"},{"location":"pages/hobbies/","text":"photography dunk amigurumi","title":"Index"},{"location":"pages/hobbies/#_1","text":"","title":""},{"location":"pages/hobbies/#photography","text":"","title":"photography"},{"location":"pages/hobbies/#dunk","text":"","title":"dunk"},{"location":"pages/hobbies/#amigurumi","text":"","title":"amigurumi"},{"location":"pages/hobbies/subpages/amigurumi/","text":"hide: - navigation - toc title: amigurumi","title":"Amigurumi"},{"location":"pages/hobbies/subpages/dunk/","text":"dunk I want dunk. jump height Date cm 01/01/01 10 max box squat Date Kg 01/01/01 110","title":"dunk"},{"location":"pages/hobbies/subpages/dunk/#dunk","text":"I want dunk.","title":"dunk"},{"location":"pages/hobbies/subpages/dunk/#jump-height","text":"Date cm 01/01/01 10","title":"jump height"},{"location":"pages/hobbies/subpages/dunk/#max-box-squat","text":"Date Kg 01/01/01 110","title":"max box squat"},{"location":"pages/hobbies/subpages/photo/","text":"I use a Canon AE1-Program with 50mm lens. Kungsladen Photos of my 2 hiking trips in the Kungsladen . Have a look Carneval of Ivrea Pretty unique folcloristic event that take place during carneval in Ivrea (Italy). I've never seen so much violence , happiness and oranges in the same place. Have a look","title":"photography"},{"location":"pages/hobbies/subpages/photo/photo_ivrea/","text":"Carneval of Ivrea 2024","title":"Carneval of Ivrea"},{"location":"pages/hobbies/subpages/photo/photo_ivrea/#carneval-of-ivrea-2024","text":"","title":"Carneval of Ivrea 2024"},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/","text":"Abisko Nikkaluotta [2023] Abisko Hemaven [2024]","title":"Kungsladen"},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/#abisko-nikkaluotta-2023","text":"","title":"Abisko Nikkaluotta [2023]"},{"location":"pages/hobbies/subpages/photo/photo_kungsladen/#abisko-hemaven-2024","text":"","title":"Abisko Hemaven [2024]"},{"location":"pages/projects/","text":"essentials","title":"Index"},{"location":"pages/projects/#_1","text":"","title":""},{"location":"pages/projects/#essentials","text":"","title":"essentials"},{"location":"pages/projects/subpages/essentials/","text":"After having worked on very different projects I've realised that still there are objects that I end up implementing every time. I'v decided to implement them once and import them from a \"general\" repository. In this page I will present main tools with few examples. Info You can find this tool box in the github repository clarkpy_essentials . To install the repository: pip install git+https://github.com/clarkmaio/clarkpy_essentials.git ParserManager Let's start from a dummy one: the ParserManager . I just do not like to edit parser from a .py file ParserManager is just a class that parse a yaml file and build a parser using the standard library argparser . Parser keys will be stored in a dictionary ready to be queried. # Example of parser_conf.yml variable_int : default : 9 help : 'This variable is an integer' variable_str : default : abc help : 'This variable is a string' variable_nargs : default : [ 'a' , 'b' , 'c' ] nargs : '*' from clarkpy_essentials import ParserManager PARSER_CONF = ` / path_to_parser / parser_conf . yml ` parser_args = ParserManager . load ( path = PARSER_CONF ) # parser_args is now just a dictionary print ( 'variable_int' , parser_args [ 'variable_int' ]) print ( 'variable_nargs' , parser_args [ 'variable_nargs' ]) DataCatalog This class is nothing but an huge if/else function to handle different type of dataset. The idea has been stolen directly from kedro than implement a very similar structure. In kedro the user write down a yaml file to map dataset to a label. The dataset can be then loaded in pipeline simply using the label as input. Since I was looking for something more explicit and a tool that could be used outside a kedro pipeline I've implemented the DataCatalog class. It is important to specify for each label the parameters: type : namely the way the dataset will be loaded. Can be pandas.csv , pandas.excel , pandas.hdf , pandas.parquet , polars.csv , polars.parquet , yaml , json , torch , pickle filepath : the path to the file. By default this is a relative path that will be appended to source_path defined in the constructor. You can refer to an absolute path using the prefix @abs: load_kwargs : additional paramters that will be passed to loader fucntion as kwargs # Example of a data catalog yml PandasDataset : type : pandas.csv filepath : dataset/csv_dataset.csv load_kwargs : sep : ',' PolarsDataset : type : polars.parquet filepath : @ abs:/home/user/dataset/parquet_dataset.parquet UrlPandasDataset : type : pandas.csv filepath : @ abs:https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv load_kwargs : sep : ',' from clarkpy_essentials import DataCatalog # Initialize the datacatalog specifying the catalog.yml path and the source path that will be used to load datasets. SOURCE_PATH = '/home/user' CATALOG_PATH = '/home/user/catalog.yml' dc = DataCatalog ( catalog = CATALOG_PATH , source_path = SOURCE_PATH ) print ( 'Pandas Dataset' , dc ( 'PandasDataset' ) . head ( 5 )) print ( 'Polars Dataset' , dc ( 'PolarsDataset' ) . head ( 5 )) print ( 'URL Pandas Dataset' , dc ( 'UrlPandasDataset' ) . head ( 5 )) Context This is a very trivial class. The only purpose it to store global senttings and variables as properties of a class. Whatever you pass to the constructor of this class will be trasformed in a property. Note catalog is a privileged key that is ment to store a DataCatalog instance. It is privileged in the sense that whatever is stored in this key will be treated as DataCatalog intance in Pipeline (see below). from clarkpy_essentials import Context global_vars = { 'GLOBAL' : 100 } context = Context ( parser = parser_args , global_vars = global_vars , catalog = dc ) print ( context . parser ) print ( context . global_vars ) print ( context . catalog ) Pipelines and nodes Again I've tried to copy a feature of kedro package but tring to make it easier to use it. I really like the way Pipelines and Nodes work in kedro, in particular the elegant way to pass input/output across different nodes. You can use mine Pipeline and Node classes to accomplish the same but you can easily use them in your program without using kedro structure. Here a dummy example: from clarkpy_essentials import Pipeline , Node , Context # ------------ Node fucntions -------------- def prod ( x : float , y : float ) -> float : return x * y def sum ( x : float , y : float ) -> float : reutrn x + y # ------------ Create Context -------------- GLOBAL_VARIABLES = { 'var1' : 1 , 'var2' : 100 , 'var3' : 10 } context = Context ( global_variables = GLOBAL_VARIABLES ) # ------------ Initialize Pipeline --------- pipeline = Pipeline ([ Node ( func = prod , inputs = [ 'context.global_variables.var1' , 'context.global_variables.var2' ], outputs = 'output_prod' ), Node ( func = sum , inputs = [ 'output_prod' , 'context.global_variables.var3' ], outputs = 'outpout_sum' ) ]) # ------------ Run Pipeline ---------------- pipeline_results = pipeline . run ( context = context ) print ( 'All pipeline variables' , pipeline_results ) For more complex example have a look at mine forecast template . DataTransformer When dealing with multiple models it happens that different data processing are needed depending on the model you are using. To make it easyier to control this operations I've created a class that take in input a list of instructions and transform a dataset accordingly. Ideally you should create instructions (in the form of a yaml ) for each model. To use correctly this class you should: Write the functions to transform data using the template: def transformer_1 ( X , ** kwars ): transform X some how return X def transformer_2 ( X , ** kwars ): transform X some how return X Create an instance of DataTransformer and map the functions associating a label dt = DataTransformer ({ 't1' : trabsformer_1 , 't2' : transformer_2 }) Create list of instructions. Each instruction step must consist in a dictionary containing the keys: type : label of the function to apply kargs [optional]: function kwargs instructions = [ { 'type' : 't1' 'kargs' : { 'kwarg1' : ... , 'kwarg2' : ... } }, 'type' : 't2' 'kargs' : { 'kwarg1' : ... , 'kwarg2' : ... } ] Transform data X_transformed = dt . transform ( X , instructions = instructions ) Here a complete dummy example: def f1 ( X ): return X def f2 ( X , alpha ): return X * alpha dt = DataTransformer () dt . add_transformer_catalog ({ 'identity' : f1 , 'mul' : f2 }) instructions = [ { 'type' : 'identity' }, { 'type' : 'mul' , 'kwargs' : { 'alpha' : 2 } } ] X = [ 1 , 2 , 3 , 4 , 5 ] X_transformed = dt . transform ( X = X , instructions = instructions ) print ( X_transformed ) Decorators @force_kwargs : raise an error if function arguments are not passed through key. @deepcopy_args : deep copy args and kwargs passed to a function.","title":"Essentials"},{"location":"pages/projects/subpages/essentials/#parsermanager","text":"Let's start from a dummy one: the ParserManager . I just do not like to edit parser from a .py file ParserManager is just a class that parse a yaml file and build a parser using the standard library argparser . Parser keys will be stored in a dictionary ready to be queried. # Example of parser_conf.yml variable_int : default : 9 help : 'This variable is an integer' variable_str : default : abc help : 'This variable is a string' variable_nargs : default : [ 'a' , 'b' , 'c' ] nargs : '*' from clarkpy_essentials import ParserManager PARSER_CONF = ` / path_to_parser / parser_conf . yml ` parser_args = ParserManager . load ( path = PARSER_CONF ) # parser_args is now just a dictionary print ( 'variable_int' , parser_args [ 'variable_int' ]) print ( 'variable_nargs' , parser_args [ 'variable_nargs' ])","title":"ParserManager"},{"location":"pages/projects/subpages/essentials/#datacatalog","text":"This class is nothing but an huge if/else function to handle different type of dataset. The idea has been stolen directly from kedro than implement a very similar structure. In kedro the user write down a yaml file to map dataset to a label. The dataset can be then loaded in pipeline simply using the label as input. Since I was looking for something more explicit and a tool that could be used outside a kedro pipeline I've implemented the DataCatalog class. It is important to specify for each label the parameters: type : namely the way the dataset will be loaded. Can be pandas.csv , pandas.excel , pandas.hdf , pandas.parquet , polars.csv , polars.parquet , yaml , json , torch , pickle filepath : the path to the file. By default this is a relative path that will be appended to source_path defined in the constructor. You can refer to an absolute path using the prefix @abs: load_kwargs : additional paramters that will be passed to loader fucntion as kwargs # Example of a data catalog yml PandasDataset : type : pandas.csv filepath : dataset/csv_dataset.csv load_kwargs : sep : ',' PolarsDataset : type : polars.parquet filepath : @ abs:/home/user/dataset/parquet_dataset.parquet UrlPandasDataset : type : pandas.csv filepath : @ abs:https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv load_kwargs : sep : ',' from clarkpy_essentials import DataCatalog # Initialize the datacatalog specifying the catalog.yml path and the source path that will be used to load datasets. SOURCE_PATH = '/home/user' CATALOG_PATH = '/home/user/catalog.yml' dc = DataCatalog ( catalog = CATALOG_PATH , source_path = SOURCE_PATH ) print ( 'Pandas Dataset' , dc ( 'PandasDataset' ) . head ( 5 )) print ( 'Polars Dataset' , dc ( 'PolarsDataset' ) . head ( 5 )) print ( 'URL Pandas Dataset' , dc ( 'UrlPandasDataset' ) . head ( 5 ))","title":"DataCatalog"},{"location":"pages/projects/subpages/essentials/#context","text":"This is a very trivial class. The only purpose it to store global senttings and variables as properties of a class. Whatever you pass to the constructor of this class will be trasformed in a property. Note catalog is a privileged key that is ment to store a DataCatalog instance. It is privileged in the sense that whatever is stored in this key will be treated as DataCatalog intance in Pipeline (see below). from clarkpy_essentials import Context global_vars = { 'GLOBAL' : 100 } context = Context ( parser = parser_args , global_vars = global_vars , catalog = dc ) print ( context . parser ) print ( context . global_vars ) print ( context . catalog )","title":"Context"},{"location":"pages/projects/subpages/essentials/#pipelines-and-nodes","text":"Again I've tried to copy a feature of kedro package but tring to make it easier to use it. I really like the way Pipelines and Nodes work in kedro, in particular the elegant way to pass input/output across different nodes. You can use mine Pipeline and Node classes to accomplish the same but you can easily use them in your program without using kedro structure. Here a dummy example: from clarkpy_essentials import Pipeline , Node , Context # ------------ Node fucntions -------------- def prod ( x : float , y : float ) -> float : return x * y def sum ( x : float , y : float ) -> float : reutrn x + y # ------------ Create Context -------------- GLOBAL_VARIABLES = { 'var1' : 1 , 'var2' : 100 , 'var3' : 10 } context = Context ( global_variables = GLOBAL_VARIABLES ) # ------------ Initialize Pipeline --------- pipeline = Pipeline ([ Node ( func = prod , inputs = [ 'context.global_variables.var1' , 'context.global_variables.var2' ], outputs = 'output_prod' ), Node ( func = sum , inputs = [ 'output_prod' , 'context.global_variables.var3' ], outputs = 'outpout_sum' ) ]) # ------------ Run Pipeline ---------------- pipeline_results = pipeline . run ( context = context ) print ( 'All pipeline variables' , pipeline_results ) For more complex example have a look at mine forecast template .","title":"Pipelines and nodes"},{"location":"pages/projects/subpages/essentials/#datatransformer","text":"When dealing with multiple models it happens that different data processing are needed depending on the model you are using. To make it easyier to control this operations I've created a class that take in input a list of instructions and transform a dataset accordingly. Ideally you should create instructions (in the form of a yaml ) for each model. To use correctly this class you should: Write the functions to transform data using the template: def transformer_1 ( X , ** kwars ): transform X some how return X def transformer_2 ( X , ** kwars ): transform X some how return X Create an instance of DataTransformer and map the functions associating a label dt = DataTransformer ({ 't1' : trabsformer_1 , 't2' : transformer_2 }) Create list of instructions. Each instruction step must consist in a dictionary containing the keys: type : label of the function to apply kargs [optional]: function kwargs instructions = [ { 'type' : 't1' 'kargs' : { 'kwarg1' : ... , 'kwarg2' : ... } }, 'type' : 't2' 'kargs' : { 'kwarg1' : ... , 'kwarg2' : ... } ] Transform data X_transformed = dt . transform ( X , instructions = instructions ) Here a complete dummy example: def f1 ( X ): return X def f2 ( X , alpha ): return X * alpha dt = DataTransformer () dt . add_transformer_catalog ({ 'identity' : f1 , 'mul' : f2 }) instructions = [ { 'type' : 'identity' }, { 'type' : 'mul' , 'kwargs' : { 'alpha' : 2 } } ] X = [ 1 , 2 , 3 , 4 , 5 ] X_transformed = dt . transform ( X = X , instructions = instructions ) print ( X_transformed )","title":"DataTransformer"},{"location":"pages/projects/subpages/essentials/#decorators","text":"@force_kwargs : raise an error if function arguments are not passed through key. @deepcopy_args : deep copy args and kwargs passed to a function.","title":"Decorators"},{"location":"pages/til/","text":"Today I learned 01.01.2023 This is a test","title":"Today I learned"},{"location":"pages/til/#today-i-learned","text":"01.01.2023 This is a test","title":"Today I learned"},{"location":"pages/til/subpages/test/","text":"Sa sa prova Today I learned how to create a TIL post and create a link from TIL page to the post it self. It was not that hard. For sure I have to work on the design.","title":"Sa sa prova"},{"location":"pages/til/subpages/test/#sa-sa-prova","text":"Today I learned how to create a TIL post and create a link from TIL page to the post it self. It was not that hard. For sure I have to work on the design.","title":"Sa sa prova"},{"location":"pages/useful%21/","text":"People koaning the idea of this website and the website itself have been copied from this man m-clark GAM God ritchie vink robj hyndman daniel linssen amazing game developer pesciolini patatini veeeeeeeeery nice paints Links matplotlib tricks","title":"Index"},{"location":"pages/useful%21/#_1","text":"","title":""},{"location":"pages/useful%21/#people","text":"koaning the idea of this website and the website itself have been copied from this man m-clark GAM God ritchie vink robj hyndman daniel linssen amazing game developer pesciolini patatini veeeeeeeeery nice paints","title":"People"},{"location":"pages/useful%21/#links","text":"matplotlib tricks","title":"Links"}]}